{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qiongqi123/DQN-Atari-pong/blob/main/DQN_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CPKB7rkl1iRn"
      },
      "outputs": [],
      "source": [
        "!pip install ale-py\n",
        "!pip install gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nAMTE60Zuck_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxkgNYA6yQX4"
      },
      "outputs": [],
      "source": [
        "# 扫描到相应的路径\n",
        "import sys\n",
        "sys.path.append('//content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nunj9zgRvNy8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from collections import namedtuple\n",
        "\n",
        "import gym, random, pickle, os.path, math, glob\n",
        "\n",
        "from wrappers import *\n",
        "\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLiwZmx4vyG0"
      },
      "outputs": [],
      "source": [
        "# from memory import ReplayMemory\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#用namedtuple()函数，定义了一个命名元组，用于存储智能体与环境交互时的经验\n",
        "Transition = namedtuple('Transion', ('state', 'action', 'next_state', 'reward'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZE6VMpov0Ec"
      },
      "outputs": [],
      "source": [
        "## 超参数\n",
        "epsilon = 0.9\n",
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1\n",
        "EPS_END = 0.02\n",
        "EPS_DECAY = 500000 #经过EPS_DECAY，epsilon由EPS_START衰减至EPS_END\n",
        "TARGET_UPDATE = 1000\n",
        "RENDER = False\n",
        "lr = 1e-3\n",
        "INITIAL_MEMORY = 10000\n",
        "MEMORY_SIZE = 10 * INITIAL_MEMORY\n",
        "n_episode = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oFqY6Gyv376"
      },
      "outputs": [],
      "source": [
        "# 这里用colab运行时的路径\n",
        "MODEL_STORE_PATH = '/content/drive/My Drive/'+'DQN_pytorch_pong'\n",
        "modelname = 'DQN_Pong'\n",
        "model_path = MODEL_STORE_PATH + '/' + 'model/' + 'DQN_Pong_episode.pt'\n",
        "\n",
        "# 本地运行时\n",
        "# MODEL_STORE_PATH = os.getcwd()\n",
        "# print(MODEL_STORE_PATH)\n",
        "# modelname = 'DQN_Pong'\n",
        "# madel_path = MODEL_STORE_PATH + '/' + 'model/' + 'DQN_Pong_episode900.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IC1AqIcxApb"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = [] #经验回放缓冲区一开始是一个空列表\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        # 在缓冲区未满时，预留一个空槽来存储新的经验。\n",
        "        # 为了防止缓冲区满了之后的下一个位置并不存在新的经验存储时，会发生索引越界错误。\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        # 循环队列的更新，当缓冲区满了之后就会从头开始覆盖\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuEjR3olxGV6"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    # 之所以初始化了参数，是为了预防漏掉某个参数使会报错，若参数被定义则会覆盖掉这里的初始化\n",
        "    def __init__(self, in_channels=4, n_actions=14):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_channels (int): 输入通道数量\n",
        "            n_actions (int): 输出动作数量，pong游戏中有14个动作\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        \"\"\"\n",
        "        使用了三个卷积层和两个全连接层，并且每个卷积层后都有批量归一化层；\n",
        "        若使用批量归一化层提高模型的稳定性和泛化能力\n",
        "        \"\"\"\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        # self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        # self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        # self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.head = nn.Linear(512, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float() / 255 # 数据归一化，将像素值归一化到[0, 1]的范围内\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.head(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JacKt2zxHG4"
      },
      "outputs": [],
      "source": [
        "class DQN_agent():\n",
        "    def __init__(self,in_channels=4, action_space=[], learning_rate=1e-3, memory_size=100000, epsilon=0.99):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          in_channels:\n",
        "          action_space:\n",
        "          learning_rate:\n",
        "          memory_size:\n",
        "          epsilon:\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        # action_space表示智能体在环境中可以采取的所有可能动作的集合，通常由Gym环境提供\n",
        "        self.action_space = action_space\n",
        "        # self.action_dim变量将存储智能体可以采取的动作数量\n",
        "        self.action_dim = self.action_space.n\n",
        "        self.memory_buffer = ReplayMemory(memory_size)\n",
        "        self.stepdone = 0\n",
        "        # 采用双网络结构，在线网络逐步更新，目标网络在一定步数后更新\n",
        "        self.DQN = DQN(self.in_channels, self.action_dim).cuda()\n",
        "        self.target_DQN = DQN(self.in_channels, self.action_dim).cuda()\n",
        "        # 加载之前训练好的模型，如果没有则提示，！模型保存这里有问题需要修改\n",
        "        if os.path.exists(model_path):\n",
        "            print(f\"Loading model from {model_path}\")\n",
        "            self.DQN.load_state_dict(torch.load(model_path))\n",
        "            self.target_DQN.load_state_dict(self.DQN.state_dict())\n",
        "        else:\n",
        "            print(f\"Model file not found at {model_path}. Starting with a new model.\")\n",
        "        # 使用RMSprop优化\n",
        "        self.optimizer = optim.RMSprop(self.DQN.parameters(),lr=learning_rate, eps=0.001, alpha=0.95)\n",
        "        # 使用随机梯度上升优化器\n",
        "        #self.optimizer = optim.SGD(self.DQN.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "        self.stepdone += 1\n",
        "        #epsilon = 0.99\n",
        "        state = state.to(device)\n",
        "        # 使用epsilon衰减的方法来平衡探索和利用\n",
        "        epsilon = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.stepdone / EPS_DECAY)\n",
        "        # print(epsilon)\n",
        "        # epsilon-greedy算法\n",
        "        if random.random()<epsilon:\n",
        "            # 从0-action_dim中随机选一个动作进行，action是一个二维的张量\n",
        "            action = torch.tensor([[random.randrange(self.action_dim)]], device=device, dtype=torch.long)\n",
        "        else:\n",
        "            action = self.DQN(state).detach().max(1)[1].view(1,1)\n",
        "            \"\"\"\n",
        "            self.DQN(state)：之前定义的DQN接受state作为输入，state是一个4维张量，根据DQN，输出是一个张量表示每个动作的Q值，eg[1,14]\n",
        "            max(1)：第一个维度上（动作维度）寻找最大值，返回一个包含最大值和相应索引的元组 eg.最大值是10.5，对应的索引是0，则返回(tensor([10.5]), tensor([0]))\n",
        "            [1]:取max(1)得到的第一个索引，变成tensor([[0]])\n",
        "            view(1,1)：将张量的形状重塑为(1,1)的二维张量\n",
        "            \"\"\"\n",
        "        return action\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        # 如果当前缓冲区的数据不够一个批量，则不操作\n",
        "        if self.memory_buffer.__len__()<BATCH_SIZE:\n",
        "            return\n",
        "        #从经验回放缓冲区中随机抽取一批经验，大小为BATCH_SIZE\n",
        "        transitions = self.memory_buffer.sample(BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        # print(batch)\n",
        "       actions = tuple((map(lambda a: torch.tensor([[a]], device='cuda'), batch.action)))\n",
        "        rewards = tuple((map(lambda r: torch.tensor([r], device='cuda'), batch.reward)))\n",
        "        # 将batch.next_state中不是终止状态（None）的next_state转化为bol的True\n",
        "        # eg. non_final_mask = torch.tensor([True, False, True, False, True], device='cuda')\n",
        "        non_final_mask = torch.tensor(\n",
        "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "            device=device, dtype=torch.uint8).bool()\n",
        "        # 将不是终止状态的next_state连接成一个张量\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to('cuda')\n",
        "        # print(type(batch.state))\n",
        "        # 将状态、动作和奖励张量分别拼接成一个大的张量\n",
        "        state_batch = torch.cat(batch.state).to('cuda')\n",
        "        action_batch = torch.cat(actions)\n",
        "        reward_batch = torch.cat(rewards)\n",
        "        # 从每个状态对应的所有动作价值估计中，提取出智能体实际采取的动作对应的价值估计。\n",
        "        state_action_values = self.DQN(state_batch).gather(1, action_batch)\n",
        "\n",
        "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "        # 将非终止状态的下一个状态价值估计赋值给 next_state_values 中对应的位置\n",
        "        next_state_values[non_final_mask] = self.target_DQN(non_final_next_states).max(1)[0].detach()\n",
        "        # 当前状态下采取实际动作的预期价值。\n",
        "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "        # print(loss)\n",
        "        self.optimizer.zero_grad() # 清空之前计算的梯度\n",
        "        loss.backward() # 反向传播计算损失函数对网络参数的梯度\n",
        "        for param in self.DQN.parameters(): # 梯度裁剪，将梯度的值限制在[-1, 1]的范围内。\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step() # 更新参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyahLyivxJ6r"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, env, agent, n_episode):\n",
        "        self.env = env\n",
        "        self.n_episode = n_episode\n",
        "        self.agent = agent\n",
        "        # self.losslist = []\n",
        "        self.rewardlist = [] #存储训练过程中每个episode获得的总奖励。\n",
        "\n",
        "    # 将Gym环境观察到的原始数据obs转换为PyTorch张量，并调整其形状，以便输入到DQN网络中。\n",
        "    # Pong游戏为例，obs是一个表示游戏画面的NumPy数组，其形状通常为(210, 160, 3)\n",
        "    def get_state(self,obs):\n",
        "        state = np.array(obs)\n",
        "        state = state.transpose((2, 0, 1))\n",
        "        state = torch.from_numpy(state)\n",
        "        return state.unsqueeze(0)    # 转化为四维的数据结构\n",
        "\n",
        "    def train(self):\n",
        "        # 之所以用900 - n_episode是因为\n",
        "        for episode in range(0,self.n_episode):\n",
        "            obs = self.env.reset() #重置Gym环境，开始一个新的episode\n",
        "            state = self.get_state(obs)\n",
        "            episode_reward = 0.0 #初始化当前 episode 的总奖励为0\n",
        "            # print('episode:',episode)\n",
        "            for t in count():\n",
        "                # print(state.shape)\n",
        "                action = self.agent.select_action(state) #得到最优的动作\n",
        "                if RENDER: #是否显示游戏画面的开关\n",
        "                    self.env.render()\n",
        "                # .step(action)方法接受一个动作作为输入，并在环境中执行这个动作，并返回四个值\n",
        "                obs,reward,done,info = self.env.step(action)\n",
        "                episode_reward += reward\n",
        "                #通过上一步得到的done判断是否结束\n",
        "                if not done:\n",
        "                    next_state = self.get_state(obs) #将新的观察数据obs转换为下一个状态next_state\n",
        "                else:\n",
        "                    next_state = None\n",
        "                # print(next_state.shape)\n",
        "                reward = torch.tensor([reward], device=device)# 将从Gym环境中获得的奖励reward转换为PyTorch张量\n",
        "                # 将四元组存到memory中\n",
        "                '''\n",
        "                state: batch_size channel h w    size: batch_size * 4\n",
        "                action: size: batch_size * 1\n",
        "                next_state: batch_size channel h w    size: batch_size * 4\n",
        "                reward: size: batch_size * 1\n",
        "                '''\n",
        "                self.agent.memory_buffer.push(state, action.to('cpu'), next_state, reward.to('cpu'))\n",
        "                state = next_state\n",
        "                # 经验池满了之后开始学习\n",
        "                if self.agent.stepdone > INITIAL_MEMORY:\n",
        "                    self.agent.learn()\n",
        "                    if self.agent.stepdone % TARGET_UPDATE == 0:\n",
        "                        # 将DQN网络的参数复制到目标网络中\n",
        "                        self.agent.target_DQN.load_state_dict(self.agent.DQN.state_dict())\n",
        "                if done:\n",
        "                    break\n",
        "                # print(episode_reward)\n",
        "                if episode % 50 == 0:\n",
        "                  # 如果目录不存在，则创建该目录\n",
        "                  os.makedirs(MODEL_STORE_PATH + '/model', exist_ok=True)\n",
        "                  torch.save(self.agent.DQN.state_dict(), MODEL_STORE_PATH + '/' + \"model/{}_episode.pt\".format(modelname))\n",
        "                  print('Total steps: {} \\t Episode: {}/{} \\t Total reward: {}'.format(self.agent.stepdone, episode, t, episode_reward))\n",
        "            # 将当前episode获得的总奖励episode_reward添加到rewardlist列表的末尾\n",
        "            self.rewardlist.append(episode_reward)\n",
        "            self.env.close()\n",
        "        return\n",
        "    def plot_reward(self):\n",
        "        plt.plot(self.rewardlist)\n",
        "        plt.xlabel(\"episode\")\n",
        "        plt.ylabel(\"episode_reward\")\n",
        "        plt.title('train_reward')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMpGopQZxM7Q"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # create environment\n",
        "    env = gym.make(\"PongNoFrameskip-v4\")\n",
        "    env = make_env(env)\n",
        "    action_space = env.action_space #通过环境导入动作空间\n",
        "    state_channel = env.observation_space.shape[2]\n",
        "    agent = DQN_agent(in_channels = state_channel, action_space= action_space)\n",
        "    trainer = Trainer(env, agent, n_episode)\n",
        "    trainer.train()\n",
        "    trainer.plot_reward()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "!pip install ale-py\n",
        "!pip install gym[atari,accept-rom-license]\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install moviepy\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ... 其他导入和超参数设置 ...\n",
        "\n",
        "# 创建虚拟显示器\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# ... DQN, DQN_agent, Trainer 类的定义 ...\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 创建环境\n",
        "    env = gym.make(\"PongNoFrameskip-v4\")\n",
        "    env = make_env(env)\n",
        "\n",
        "    # ... 其他代码 ...\n",
        "\n",
        "    frames = []  # 存储每一帧的画面\n",
        "\n",
        "    trainer = Trainer(env, agent, n_episode)\n",
        "    trainer.train()\n",
        "\n",
        "    # 在训练过程中记录画面\n",
        "    for episode in range(900, trainer.n_episode):\n",
        "        obs = env.reset()\n",
        "        state = trainer.get_state(obs)\n",
        "\n",
        "        for t in count():\n",
        "            action = agent.select_action(state)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            frames.append(env.render('rgb_array'))  # 记录画面\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # ... 其他代码 ...\n",
        "\n",
        "    # 关闭环境和虚拟显示器\n",
        "    env.close()\n",
        "    display.stop()\n",
        "\n",
        "    # 创建视频\n",
        "    clip = ImageSequenceClip(frames, fps=30)\n",
        "    clip.write_videofile(\"game_video.mp4\")\n",
        "这段代码在训练过程中将每一帧的画面添加到frames列表中，\n",
        "并在训练结束后将这些画面合成为一个视频文件game_video.mp4。\n",
        "你可以在Colab的文件管理器中找到这个视频文件，并下载到本地观看。\n",
        "'''"
      ],
      "metadata": {
        "id": "rYmpfFI3nsgi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPi8arK/d2z/9WwVEaOF56l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
